{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlined framework of EDA for classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was designed to streamline the explorarory data analysis of common datasets that contained categorical, numeric and time-based data. The steps below focus on the common cleaning and feature engineering done on datasets to prep them for ML models. \n",
    "\n",
    "Since 70% of time taken in data science projects focus on EDA, this tries to recuce time by passing over the commmon steps taken. This was specifically not attuned to object-oriented programming as each cell has its own specific feature that needs to be monitored before continuing with the next step.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts taken from: \n",
    "\n",
    "The Essential Toolbox for Data Cleaning  - https://towardsdatascience.com/the-simple-yet-practical-data-cleaning-codes-ad27c4ce0a38\n",
    "\n",
    "Code snippets from CHRIS ALBON-\n",
    "https://chrisalbon.com/\n",
    "\n",
    "\n",
    "Data Cleaning with Python - https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d\n",
    "\n",
    "EDA for Feature Selection in ML - http://services.google.com/fh/files/misc/exploratory_data_analysis_for_feature_selection_in_machine_learning.pdf\n",
    "\n",
    "Pythonic Data Cleaning With Pandas and NumPy - https://realpython.com/python-data-cleaning-numpy-pandas/\n",
    "\n",
    "Tour of Evaluation Metrics for Imbalanced Classification - https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "\n",
    "A Starter Pack to EDA - https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE \n",
    "Some of the topics have been color coded so as to emphasize its use in the code\n",
    "\n",
    "<span style=\"color:red\">**RED**</span> - Warnings before using <p><p> \n",
    "<span style=\"color:blue\">**BLUE**</span> - An option to choose from <p><p>\n",
    "<span style=\"color:green\">**GREEN**</span> - Experimental development - would like some insights <p><p>\n",
    "\n",
    "\n",
    "## <span style=\"color:red\">**IMPORTANT**</span>\n",
    "\n",
    "Features  which require manual addtion of specific column variables are referenced as **col1** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import scipy.stats as ss\n",
    "\n",
    "from datetime import date, time\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_seq_items',None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data [CSV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas has a lot more features for reading csv data \n",
    "\n",
    "pd.read_csv(*filepath_or_buffer*, *sep=’*, ‘, delimiter=None, header=’infer’, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, *nrows=None*, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=’infer’, thousands=None, decimal=b’.’, lineterminator=None, quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('col1', sep=';',nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.figure_factory as ff\n",
    "# figure = ff.create_scatterplotmatrix(df,diag='histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Find and drop duplicated values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Check if the duplicated values mean anything significant**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate count statistics of duplicate entries\n",
    "if len(df[df.duplicated()]) > 0:\n",
    "    print(\"No. of duplicated entries: \", len(df[df.duplicated()]))\n",
    "    print(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head())\n",
    "else:\n",
    "    print(\"No duplicated entries found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the duplicate\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-depth analysis on missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    pct_missing = np.mean(df[col].isnull())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">**(1) Drop Missing Rows**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with a lot of missing values.\n",
    "ind_missing = df[df['col1'] > 35].index\n",
    "df_less_missing_rows = df.drop(ind_missing, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">**(2) Drop Missing Feature**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the feature entirely\n",
    "cols_to_drop = ['col1']\n",
    "df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">**Imputation is done later when we have a better idea about the numeric data**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = df['col1']\n",
    "df.drop(labels=['col1'], axis=1,inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">**Check if the target variable needs to be encoded.**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "target = le.fit_transform(target)\n",
    "target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Statistical Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Check for imbalances in the classifiers**</span>\n",
    "\n",
    "Each method is applicable based on the dataset.\n",
    "Taken from The Machine Learning Mastery website - Evaluation Metrics for Imbalanced Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><strong>Are you predicting probabilities?</strong>\n",
    "<ul>\n",
    "<li><strong>Do you need class labels?</strong>\n",
    "<ul>\n",
    "<li><strong>Is the positive class more important?</strong>\n",
    "<ul>\n",
    "<li>Use Precision-Recall AUC</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Are both classes important?</strong>\n",
    "<ul>\n",
    "<li>Use ROC AUC</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Do you need probabilities?</strong>\n",
    "<ul>\n",
    "<li>Use Brier Score and Brier Skill Score</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Are you predicting class labels?</strong>\n",
    "<ul>\n",
    "<li><strong>Is the positive class more important?</strong>\n",
    "<ul>\n",
    "<li><strong>Are False Negatives and False Positives Equally Important?</strong>\n",
    "<ul>\n",
    "<li>Use F1-Measure</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Are False Negatives More Important?</strong>\n",
    "<ul>\n",
    "<li>Use F2-Measure</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Are False Positives More Important?</strong>\n",
    "<ul>\n",
    "<li>Use F0.5-Measure</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Are both classes important?</strong>\n",
    "<ul>\n",
    "<li><strong>Do you have &lt; 80%-90% Examples for the Majority Class?&nbsp;</strong>\n",
    "<ul>\n",
    "<li>Use Accuracy</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Do you have &gt; 80%-90% Examples for the Majority Class?&nbsp;</strong>\n",
    "<ul>\n",
    "<li>Use G-Mean</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-wise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values= df.nunique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks for columns with varying data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(df.index)\n",
    "low_information_cols = [] #\n",
    "\n",
    "for col in df.columns:\n",
    "    cnts = df[col].value_counts(dropna=False)\n",
    "    top_pct = (cnts/num_rows).iloc[0]\n",
    "    \n",
    "    if top_pct < 0.75 and top_pct > 0.20:\n",
    "        low_information_cols.append(col)\n",
    "        print('{0}: {1:.5f}%'.format(col, top_pct*100))\n",
    "        print(cnts)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for unique analysis\n",
    "for i in df.columns:\n",
    "    plt.figure()\n",
    "    plt.title(f'{i} - {unique_values[i]} unique values')\n",
    "    values=pd.value_counts(df[i]).plot.bar()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the index of the dataframe as a unique column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with unique values\n",
    "unique_columns =[index for  index,values in unique_values.items() if values==len(df) ]\n",
    "print(f'{unique_columns if unique_columns !=[] else None}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('col1',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert numeric to categorical\n",
    "\n",
    "Sometimes categorical values are denoted as numeric (0 and 1). It is better to label them as categorical before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical=[\"col1\"]\n",
    "df[to_categorical].astype(cat_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of summary based on the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_values = df.groupby(target)\n",
    "grouped_summary=grouped_values.mean()\n",
    "grouped_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_columns={'col1':'New_col1'}\n",
    "df.rename(columns=replace_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the target variable from the dataframe\n",
    "\n",
    "Since we have a target stored as a variable, it is better to seperate it from the rest of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=target.name,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get numeric and categorical features\n",
    "\n",
    "NOTE: Categorical features will also contain timestamps as there may be a chance that it exists as a string datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric=df.select_dtypes(include=['float64', 'int64'])\n",
    "df_categorical=df.select_dtypes(exclude=['float64', 'int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra white space in text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_categorical.columns:\n",
    "    df_categorical[col] = df_categorical[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plot of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_categorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n",
    "    '''\n",
    "    Helper function that gives a quick summary of a given column of categorical data\n",
    "    Arguments\n",
    "    =========\n",
    "    dataframe: pandas dataframe\n",
    "    x: str. horizontal axis to plot the labels of categorical data, y would be the count\n",
    "    y: str. vertical axis to plot the labels of categorical data, x would be the count\n",
    "    hue: str. if you want to compare it another variable (usually the target variable)\n",
    "    palette: array-like. Colour of the plot\n",
    "    Returns\n",
    "    =======\n",
    "    Quick Stats of the data and also the count plot\n",
    "    '''\n",
    "    if x == None:\n",
    "        column_interested = y\n",
    "    else:\n",
    "        column_interested = x\n",
    "    series = dataframe[column_interested]\n",
    "    print ('\\t', column_interested)\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n",
    "    plt.show()\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_categorical.columns:\n",
    "    categorical_summarized(df_categorical,y=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_categorical.columns:\n",
    "    categorical_summarized(df_categorical,y=col,hue=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing categorical values with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each non-numeric column.\n",
    "non_numeric_cols = df_categorical.columns.values\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        \n",
    "        top = df[col].describe()['top'] # impute with the most frequent value.\n",
    "        df[col] = df[col].fillna(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi square analysis\n",
    "\n",
    "Tests for independence of categorical variables. Requires all categorical variables to be **label encoded** before analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi_square = df_categorical.apply(LabelEncoder().fit_transform)\n",
    "chi_scores = chi2(chi_square,target)\n",
    "chi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = chi_square.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "p_values.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Choose relevant features and encoding methods**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">** (1) Label Encoder **</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df_categorical['col1'])         \n",
    "le.transform(df_categorical['col1'])         # Convert to Integer values\n",
    "\n",
    "## Add to df_categorical\n",
    "df_categorical.replace(le, inplace=True) \n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">** (2) Label Encoder with Weights  **</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encode = {'col1' : {'YES':1, 'NO':0},\n",
    "              'col1'  : {'WON':1, 'LOSE':0, 'DRAW':0}}  \n",
    "df_categorical.replace(num_encode, inplace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">** (3) One Hot Encoder  **</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_variables=['col1','col2']\n",
    "one_hot_encode=pd.get_dummies(data=df_categorical, columns=one_hot_variables,dummy_na=True)\n",
    "#df_categorical.drop([one_hot_variables])\n",
    "df_categorical=pd.concat([df_categorical, one_hot_encode], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-Time data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Make sure that the data related to time is a relevant feature (Classification models cannot use datetime variables)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert timestamp(from string to datetime format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical['timestamp']=pd.to_datetime(df_categorical['col'], format='%Y-%m-%d %H:%M:%S.%f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Day, month year if the timestamp is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d')\n",
    "df_categorical['year'] = df_categorical['date'].dt.year\n",
    "df_categorical['month'] = df_categorical['date'].dt.month\n",
    "df_categorical['day'] = df_categorical['date'].dt.day\n",
    "df['weekday'] = df['timestamp_dt'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical['hour'] = df_categorical['date'].dt.hour\n",
    "df_categorical['minute'] = df_categorical['date'].dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Days Of The Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical['weekday'] = df_categorical.date.weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between two dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical['date_difference']=abs((d2 - d1).days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the orginal timestamp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_categorical['col1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.hist(figsize=(20, 20), bins=10, xlabelsize=8, ylabelsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Statistical Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantitative_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', ax=None, verbose=True, swarm=False):\n",
    "    '''\n",
    "    Helper function that gives a quick summary of quantattive data\n",
    "    Arguments\n",
    "    =========\n",
    "    dataframe: pandas dataframe\n",
    "    x: str. horizontal axis to plot the labels of categorical data (usually the target variable)\n",
    "    y: str. vertical axis to plot the quantitative data\n",
    "    hue: str. if you want to compare it another categorical variable (usually the target variable if x is another variable)\n",
    "    palette: array-like. Colour of the plot\n",
    "    swarm: if swarm is set to True, a swarm plot would be overlayed\n",
    "    Returns\n",
    "    =======\n",
    "    Quick Stats of the data and also the box plot of the distribution\n",
    "    '''\n",
    "    series = dataframe[y]\n",
    "    print ('\\t', y)\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.boxplot(x=x, y=y, hue=hue, data=dataframe, palette=palette, ax=ax,)\n",
    "\n",
    "    if swarm:\n",
    "        sns.swarmplot(x=x, y=y, hue=hue, data=dataframe,\n",
    "                      palette=palette, ax=ax)\n",
    "\n",
    "    plt.show()\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate numeric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_numeric.columns:\n",
    "    quantitative_summarized(df_numeric,y=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate numeric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_numeric.columns:\n",
    "    quantitative_summarized(dataframe= df_numeric, y = col, x = target, palette=c_palette, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df_numeric[~((df_numeric < (Q1 - 1.5 * IQR)) |(df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate numeric analysis\n",
    "\n",
    "Use to see which 2 columns that sould be analyzed based on target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantitative_summarized(dataframe= df_numeric, y = 'col1', x = target, hue = 'col1', palette=c_palette, verbose=False, swarm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q1 = df_numeric.quantile(0.25)\n",
    "Q3 = df_numeric.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_numeric_outliers = df_numeric[((df_numeric < (Q1 - 1.5 * IQR)) |(df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df_numeric_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df_numeric[~((df_numeric < (Q1 - 1.5 * IQR)) |(df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe after eliminating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_numeric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewedness and Kurtosis distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewedness\n",
    "\n",
    "* If skewness is less than −1 or greater than +1, the distribution is highly skewed.\n",
    "* If skewness is between −1 and −½ or between +½ and +1, the distribution is moderately skewed.\n",
    "* If skewness is between −½ and +½, the distribution is approximately symmetric.\n",
    "\n",
    "### Kurtosis \n",
    "\n",
    "* A normal distribution has kurtosis exactly 3 (excess kurtosis exactly 0). Any distribution with kurtosis ≈3 (excess ≈0) is called mesokurtic.\n",
    "* A distribution with kurtosis <3 (excess kurtosis <0) is called platykurtic. Compared to a normal distribution, its tails are shorter and thinner, and often its central peak is lower and broader.\n",
    "* A distribution with kurtosis >3 (excess kurtosis >0) is called leptokurtic. Compared to a normal distribution, its tails are longer and fatter, and often its central peak is higher and sharper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_orientation = pd.DataFrame({\"Skewedness\":df_numeric.skew(),\"Kurtosis\":df_numeric.kurt()})\n",
    "df_numeric_orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce memory load by converting to 32-bits\n",
    "\n",
    "<span style=\"color:red\">**Confirm that the data values do not matter when downsizing**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dtypes (int64 -> int32)\n",
    "df_numeric[['col1']] = df_numeric[['col1']].astype('int32')\n",
    "\n",
    "# Change the dtypes (float64 -> float32)\n",
    "df_numeric[['col1']] = df_numeric[['col1']].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute all missing numeric data with Median \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each numeric column.\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = df_numeric[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print(f'imputing missing values for: {col} - Missing {num_missing} values')\n",
    "\n",
    "        med = df_numeric[col].median()\n",
    "        df_numeric[col] = df_numeric[col].fillna(med)\n",
    "\n",
    "print(\"All collumns checked\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find qunatiles for each numeric value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_check=df_numeric.quantile([.1, .25, .5, .75], axis = 0)\n",
    "quantile_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['col1']\n",
    "scaled_variables = df_numeric[cols].values.astype(float)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaled = min_max_scaler.fit_transform(scaled_variables)\n",
    "min_max_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_cols = [f'min_max_{c}' for c in cols]\n",
    "df_numeric=pd.concat([df_numeric, pd.DataFrame(min_max_scaled, columns=bc_cols)], axis='columns')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Visualization\n",
    "df_numeric[cols].hist(layout=(2, 6), figsize=(15, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min_max transformed\n",
    "df_numeric[bc_cols].hist(layout=(2, 6), figsize=(15, 4), color='orange', alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose to drop the original\n",
    "df_numeric.drop(['col1'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yeo-Johnson transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['col1']\n",
    "scaled_variables = df_numeric[cols].values.astype(float)\n",
    "yeo_johnson_scaler = PowerTransformer(method='yeo-johnson')\n",
    "yeo_scaled = yeo_johnson_scaler.fit_transform(scaled_variables)\n",
    "yeo_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_cols = [f'yeo_johnson_{c}' for c in cols]\n",
    "df_numeric=pd.concat([df_numeric, pd.DataFrame(yeo_scaled, columns=bc_cols)], axis='columns')\n",
    "ds.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Visualization\n",
    "df_numeric[cols].hist(layout=(2, 6), figsize=(15, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yeo-Johnsontransformed\n",
    "df_numeric[bc_cols].hist(layout=(2, 6), figsize=(15, 4), color='orange', alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose to drop the original\n",
    "df_numeric.drop(['col1'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame transformations\n",
    "\n",
    "<span style=\"color:red\">**Make sure the relevant columns are transformed accordingly**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new colum based on the sum of two values\n",
    "df_numeric['Total_col'] = df_numeric['col1'] + df_numeric['col2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target= pd.DataFrame(target,columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined= pd.concat([df_numeric, df_categorical,df_target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if there are any null values in the final dataframe.\n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.dropna(axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the overall dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">**(1) As a CSV**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"transformed_data.csv\",index=False)\n",
    "print(\"Dataframe stored as a CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">**(2) As a pickle**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_pickle(\"./transformed_data.pkl\")\n",
    "print(\"Dataframe stored as a pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
